# path to SemanticTHAB dataset (sequences), no tailing path seperator!
dataset_dir: "/home/devuser/workspace/data/semantic_datasets/SemanticTHAB/sequences"
dataset_name: "SemanticTHAB"

model_settings: {
  # Defines model backbone architecture
  # Baseline model. type: str. options: SalsaNext | Reichert
    baseline: "Reichert", 
  # Backbone architecture
    model_type: "efficientnet_v2_l",
  # Loss function. type: str. options: Tversky | CE | Lovasz | Dirichlet | SalsaNext
    loss_function: "Dirichlet",
  # Model Layer settings
  ## toggle attention block. type: bool
    attention: true,
  ## inject meta data at multiple scales. type: bool
    multi_scale_meta: true,
  
  # Define what optional input channel types are used
    # range (1 channel) and xyz (3 channels) are always considered -> min.4 channels
  ## toggle use of normals as input (+3 extra channels). type: bool
    normals: true,
  ## toggle use of reflectivity as input (+1 extra channels). type: bool
    reflectivity: true,

  # AUGMENTATIONS
  ## rotation augmentation. type: bool
    rotate: false,    # TODO: currently too slow
  ## flip augmentation. type: bool
    flip: true,
  
  # Toggle use of Dropout at inference
  use_mc_sampling: false,
  mc_samples:  50,

  ## Image projection resolution
    projection: [128, 2048],
  # GENERAL SETTINGS
  ## path to pretrained model weights, if no pretrained option is desired set to pretrained: null
    ## type: str or null (NoneType), no tailing path seperator! expects .pt or .pth file
    pretrained: null #"/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticTHAB/Reichert/Dirichlet_nram/25-10-14_23-56-50/model_final.pt"
    # Salsanext Salsanext loss: "/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticTHAB/SalsaNext/test_split_0008/25-10-13_16-06-59/model_final.pt"
    #Reichert dirichlet: "/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticTHAB/Reichert/test_split_0008/Dirichlet_nram/25-10-13_16-13-57/model_final.pt",
}

model_weights: {
  Dirichlet: {
    w_ls:    0.0,   # add only if you want extra IoU (on softmax probs)
    w_nll:   1.0,  # Dirichlet-categorical NLL or density based
    w_imax:  0.0,   # 2.0
    w_dce:   0.0,   # 1.0
    w_kl:    0.0,  # KL evidence term governs alpha0, keeps near prior_concentration
    w_ir:    0.0,  # information regularization term, keeps off-class near 1
    w_comp:  1.0,    # shapes uncertainty on unsure pixels, 5.0
    w_brier: 0.5,  # 0.05
    w_evid_reg: 0.75,  # 1.e-1 for log_squared, 1.e-5 for one_sided
    w_logit_reg: 3.e-4, # 1.e-4, 5.e-5
    target_shares: {"nll": 0.85, "ls": 0.0, "dce": 0.0, "imax": 0.0, "comp": 0.05, "brier": 0.10}
  },
  SalsaNext: {
    w_nll:   1.0, 
    w_ls:    1.0
  },
  Tversky: {
    w_ce:         1.0, 
    w_tversky:    1.0
  }
}

# on 24GB VRAM tested: batch_size: 4, num_workers: 8
# on 12GB VRAM tested: batch_size: 1, num_workers: 16
train_params: {
  ## learning rate for training
    learning_rate: 5.e-4,
  ## Sets batch size for training
    batch_size: 4,  # 4
  ## Sets numbor of worker processes
    num_workers: 8,
  ## Sets number of total training epochs
    num_epochs: 50,
  ## Sets weight decay
  weight_decay: 1.e-4,
  ## Gradually/Linearilly increases learning rate until base learning is reached for 'num_warmup_epochs' Epochs
  num_warmup_epochs: 2

}

logging_settings: {
  ## Test ID of the test sequence for the leave one out CV. -1 for training on all
    test_id: 8,
  ## Sets interval to test every nth epoch
    test_every_nth_epoch: 2,
  ## Sets interval to save model weights every nth epoch. -1 for saving only last epoch
    save_every_nth_epoch: 2,
  ## path to log directory
    log_dir: "/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticTHAB",
    metrics: {
      McConfEmpAcc: false, # bool, MC Mean Confidence vs. Empirical Most-Likely Accuracy Reliability Plot
      EntErrRel: false, # bool, entropy error reliability diagram, plots at which entropy levels the model's uncertainty matches its true error frequency
      IouPlt: true
    }
}