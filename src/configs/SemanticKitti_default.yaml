# path to SemanticTHAB dataset (sequences), no tailing path seperator!
dataset_dir: "/home/devuser/workspace/data/semantic_datasets/SemanticKitti/dataset/sequences"
dataset_name: "SemanticKitti"

model_settings: {
  # Defines model backbone architecture
  # Baseline model. type: str. options: SalsaNext | Reichert
    baseline: "Reichert", 
  # Loss function. type: str. options: Tversky | CE | Lovasz | Dirichlet | SalsaNext
    loss_function: "Dirichlet",

  # Model Layer settings, specific to "Reichert" baseline
    ## Backbone architecture. Note: Reichert only
      model_type: "efficientnet_v2_l",
    ## toggle attention block. type: bool. Note: Reichert only
      attention: true,
    ## inject meta data at multiple scales. type: bool. Note: Reichert only
      multi_scale_meta: true,
  
  # Define what optional input channel types are used
    ## range (1 channel) and xyz (3 channels) are always considered -> min.4 channels
  ## toggle use of normals as input (+3 extra channels). type: bool
    normals: true,
  ## toggle use of reflectivity as input (+1 extra channels). type: bool
    reflectivity: true,

  # AUGMENTATIONS
  ## rotation augmentation. type: bool
    rotate: false,    # TODO: currently too slow, do NOT turn on!
  ## flip augmentation. type: bool
    flip: true,
  
  # Toggle use of Dropout at inference
  use_mc_sampling: false,
  mc_samples:  50,

  ## Image projection resolution
    projection: [64, 2048], # [64, 2048]
  # GENERAL SETTINGS
  ## path to pretrained model weights, if no pretrained option is desired set to pretrained: null
    ## type: str or null (NoneType), no tailing path seperator! expects .pt or .pth file
    pretrained: null, #"/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticKitti/Reichert/Dirichlet_nram/25-10-17_09-53-58/weights/best_epoch_004.pt"
    # SalsaNext: "/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticKitti/SalsaNext/test_split_0008/efficientnet_v2_l_anmSalsaNext/25-10-02_12-34-06/model_final.pt"
    # Reichert Dirchlet: "/home/devuser/workspace/data/train_semantic_THAB_v3/test_split_final/efficientnet_v2_l_anmpDirichlet/25-07-01_17-13-35/model_000001.pt"
    # Reichert pretrained: "/home/devuser/workspace/data/train_semantic_THAB_v3/efficientnet_v2_l_anmpTversky/model_final.pth"
}

model_weights: {
  Dirichlet: {
    w_ls:    0.0,   # add only if you want extra IoU (on softmax probs)
    w_nll:   1.0,  # Dirichlet-categorical NLL or density based
    w_imax:  0.0,   # 2.0
    w_dce:   0.0,   # 1.0
    w_kl:    0.0,  # KL evidence term governs alpha0, keeps near prior_concentration
    w_ir:    0.0,  # information regularization term, keeps off-class near 1
    w_comp:  1.0,    # shapes uncertainty on unsure pixels, 5.0
    w_brier: 0.5,  # 0.05
    w_evid_reg: 0.75,  # 1.e-1 for log_squared, 1.e-5 for one_sided
    w_logit_reg: 3.e-4, # 1.e-4, 5.e-5
    target_shares: {"nll": 0.85, "ls": 0.0, "dce": 0.0, "imax": 0.0, "comp": 0.05, "brier": 0.10}
  },
  SalsaNext: {
    w_nll:   1.0, 
    w_ls:    1.0
  },
  Tversky: {
    w_ce:         1.0, 
    w_tversky:    1.0
  }
}
  
# at 64,512: batch_size: 16, num_workers: 8
# at 64,2048: batch_size: 6, num_workers: 16
# for SalsaNextAdf at 64,2048: batch_size: 1, num_workers: 16
# for SalsaNext at 64, 2048: batch_size: 8, num_workers: 16
train_params: {
  ## learning rate for training
    learning_rate: 5.e-4,
  ## min LR
    learning_rate_min: 5.e-6,
  ## Sets batch size for training
    batch_size: 8,
  ## Sets numbor of worker processes
    num_workers: 16,
  ## Sets number of total training epochs
    num_epochs: 50, # using CosineAnnealingWarmRestarts LR scheduler with length 15epochs each cycle -> 15*4=60
  ## Sets weight decay
  weight_decay: 1.e-4,
  ## Gradually/Linearilly increases learning rate until base learning is reached for 'num_warmup_epochs' Epochs
  num_warmup_epochs: 2

}

logging_settings: {
  ## Test ID of the test sequence for the leave one out CV. -1 for training on all
    test_id: 8,
  ## Sets interval to test every nth epoch
    test_every_nth_epoch: 2,
  ## Sets interval to save model weights every nth epoch. -1 for saving only last epoch
    save_every_nth_epoch: 2,
  ## path to log directory
    log_dir: "/home/devuser/workspace/data/Training_SemanticSegmentation/train_semanticKitti",
    metrics: {
      McConfEmpAcc: false, # bool, MC Mean Confidence vs. Empirical Most-Likely Accuracy Reliability Plot
      EntErrRel: false, # bool, entropy error reliability diagram, plots at which entropy levels the model's uncertainty matches its true error frequency
      IouPlt: true
    }
}
